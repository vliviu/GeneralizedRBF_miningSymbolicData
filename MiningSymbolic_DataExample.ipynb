{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809f42e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db052975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticData:\n",
    "    def __init__(self, values, probabilities):\n",
    "        \"\"\"\n",
    "        Initialize probabilistic data with values and their associated probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        - values: List of values (e.g., numbers, categories).\n",
    "        - probabilities: List of probabilities corresponding to each value.\n",
    "        \"\"\"\n",
    "        if len(values) != len(probabilities):\n",
    "            raise ValueError(\"Values and probabilities must have the same length.\")\n",
    "        \n",
    "        if not np.isclose(sum(probabilities), 1):\n",
    "            raise ValueError(\"Probabilities must sum to 1.\")\n",
    "        \n",
    "        self.values = np.array(values)\n",
    "        self.probabilities = np.array(probabilities)\n",
    "\n",
    "    def expected_value(self):\n",
    "        \"\"\"\n",
    "        Calculate the expected value of the probabilistic data.\n",
    "        \n",
    "        Returns:\n",
    "        - The expected value.\n",
    "        \"\"\"\n",
    "        return np.sum(self.values * self.probabilities)\n",
    "\n",
    "    def sample(self, size=1):\n",
    "        \"\"\"\n",
    "        Sample from the probabilistic distribution.\n",
    "        \n",
    "        Parameters:\n",
    "        - size: Number of samples to draw.\n",
    "        \n",
    "        Returns:\n",
    "        - A list of sampled values.\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.values, size=size, p=self.probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf715130",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mprobabilities[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m data\u001b[38;5;241m.\u001b[39mprobabilities[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m predicted_probabilities]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m(true_labels, predicted_labels)\n\u001b[0;32m     26\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(true_labels, predicted_labels)\n\u001b[0;32m     27\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(true_labels, predicted_labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage for classification\n",
    "if __name__ == \"__main__\":\n",
    "    # Define some probabilistic data for a binary classification scenario\n",
    "    # Let's say we have two classes: 0 and 1\n",
    "    true_labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "    \n",
    "    # Simulate probabilistic predictions\n",
    "    predicted_probabilities = [\n",
    "        ProbabilisticData([0, 1], [0.7, 0.3]),  # Predicted probabilities for the first sample\n",
    "        ProbabilisticData([0, 1], [0.2, 0.8]),  \n",
    "        ProbabilisticData([0, 1], [0.9, 0.1]),  \n",
    "        ProbabilisticData([0, 1], [0.1, 0.9]),  \n",
    "        ProbabilisticData([0, 1], [0.6, 0.4]),  \n",
    "        ProbabilisticData([0, 1], [0.4, 0.6]),  \n",
    "        ProbabilisticData([0, 1], [0.8, 0.2]),  \n",
    "        ProbabilisticData([0, 1], [0.3, 0.7]),  \n",
    "        ProbabilisticData([0, 1], [0.5, 0.5]),  \n",
    "        ProbabilisticData([0, 1], [0.6, 0.4]),  \n",
    "    ]\n",
    "    \n",
    "    # Convert probabilistic predictions to class labels\n",
    "    predicted_labels = [1 if data.probabilities[1] > data.probabilities[0] else 0 for data in predicted_probabilities]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffbfec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
